{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sMElZrFMYa_L",
        "outputId": "26a353be-b064-44c6-b79d-541c3a4c0145"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.13), please consider upgrading to the latest version (0.4.3).\n",
            "Data source import complete.\n"
          ]
        }
      ],
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "import kagglehub\n",
        "adityajn105_flickr30k_path = kagglehub.dataset_download('adityajn105/flickr30k')\n",
        "\n",
        "print('Data source import complete.')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MdGlyj21Ya_Q"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import os, pickle, torch, torch.nn as nn\n",
        "from torchvision import models, transforms\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lroKqbvRYa_S",
        "outputId": "13b94df6-fcb0-45c3-e17d-5502f3f2742f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU is running..\n"
          ]
        }
      ],
      "source": [
        "if torch.cuda.is_available():\n",
        "    print(\"GPU is running and it's name is: \", torch.cuda.get_device_name(0))\n",
        "else:\n",
        "    print('CPU is running..')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IjRqP-wwYa_T",
        "outputId": "460dc4b2-49c9-4998-f33a-e4f0f1cf9c12"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Found images at: /root/.cache/kagglehub/datasets/adityajn105/flickr30k/versions/1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting Features: 0it [00:00, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Success! 0 images processed and saved to flickr30k_features.pkl\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# now first understand the Assignment (understood the problem)\n",
        "\n",
        "\n",
        "def find_image_dir():\n",
        "    # Common Kaggle root\n",
        "    base_input = '/kaggle/input'\n",
        "\n",
        "    # Walk through the input directory to find where the images actually are\n",
        "    for root, dirs, files in os.walk(base_input):\n",
        "        # Look for the folder containing a high volume of jpg files\n",
        "        if len([f for f in files if f.endswith('.jpg')]) > 1000:\n",
        "            return root\n",
        "    return None\n",
        "\n",
        "\n",
        "# IMAGE_DIR = find_image_dir() # Original line, replaced to use correct path\n",
        "IMAGE_DIR = adityajn105_flickr30k_path # Use the path provided by kagglehub\n",
        "OUTPUT_FILE = 'flickr30k_features.pkl'\n",
        "\n",
        "if IMAGE_DIR:\n",
        "    print(f\" Found images at: {IMAGE_DIR}\")\n",
        "else:\n",
        "    raise FileNotFoundError(\n",
        "        \"Could not find the Flickr30k image directory. Please ensure the dataset is added to the notebook.\"\n",
        "    )\n",
        "\n",
        "\n",
        "# --- THE DATASET CLASS ---\n",
        "class FlickrDataset(Dataset):\n",
        "    def __init__(self, img_dir, transform):\n",
        "        self.img_names = [\n",
        "            f for f in os.listdir(img_dir)\n",
        "            if f.endswith(('.jpg', '.jpeg'))\n",
        "        ]\n",
        "        self.transform = transform\n",
        "        self.img_dir = img_dir\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_names)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        name = self.img_names[idx]\n",
        "        img_path = os.path.join(self.img_dir, name)\n",
        "        img = Image.open(img_path).convert('RGB')\n",
        "        return self.transform(img), name\n",
        "\n",
        "\n",
        "# --- REMAINDER OF THE PIPELINE (AS BEFORE) ---\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "model = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
        "model = nn.Sequential(*list(model.children())[:-1])  # Feature vector only\n",
        "model = nn.DataParallel(model).to(device)\n",
        "model.eval()\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(\n",
        "        (0.485, 0.456, 0.406),\n",
        "        (0.229, 0.224, 0.225)\n",
        "    )\n",
        "])\n",
        "\n",
        "dataset = FlickrDataset(IMAGE_DIR, transform)\n",
        "loader = DataLoader(dataset, batch_size=128, num_workers=4,)\n",
        "\n",
        "features_dict = {}\n",
        "\n",
        "with torch.no_grad():\n",
        "    for imgs, names in tqdm(loader, desc=\"Extracting Features\"):\n",
        "        feats = model(imgs.to(device)).view(imgs.size(0), -1)\n",
        "        for i, name in enumerate(names):\n",
        "            features_dict[name] = feats[i].cpu().numpy()\n",
        "\n",
        "with open(OUTPUT_FILE, 'wb') as f:\n",
        "    pickle.dump(features_dict, f)\n",
        "\n",
        "print(f\"Success! {len(features_dict)} images processed and saved to {OUTPUT_FILE}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HsWBuvOSYa_T"
      },
      "outputs": [],
      "source": [
        "CAPTIONS_FILE = \"/kaggle/input/flickr30k/captions.txt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "msYNkk0TYa_U",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 183
        },
        "outputId": "7150f972-cb50-4931-c6e9-21e047583372"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/kaggle/input/flickr30k/captions.txt'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2938792552.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCAPTIONS_FILE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/input/flickr30k/captions.txt'"
          ]
        }
      ],
      "source": [
        "with open(CAPTIONS_FILE, 'r') as f:\n",
        "    for i in range(5):\n",
        "        print(f.readline())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M0IsbMO3Ya_V"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from collections import defaultdict\n",
        "\n",
        "def clean_caption(caption):\n",
        "    caption = caption.lower()\n",
        "    caption = re.sub(r\"[^a-z\\s]\", \"\", caption)\n",
        "    caption = caption.strip()\n",
        "    return f\"<start> {caption} <end>\"\n",
        "\n",
        "# Dictionary: image_name -> list of captions\n",
        "image_captions = defaultdict(list)\n",
        "\n",
        "with open(CAPTIONS_FILE, 'r') as f:\n",
        "    next(f)  # skip header line: image,caption\n",
        "    for line in f:\n",
        "        line = line.strip()\n",
        "        if not line:\n",
        "            continue\n",
        "\n",
        "        image_name, caption = line.split(',', 1)  # split ONLY first comma\n",
        "        caption = clean_caption(caption)\n",
        "        image_captions[image_name].append(caption)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OnpfJPggYa_V"
      },
      "outputs": [],
      "source": [
        "for image, captions in image_captions.items():\n",
        "    print(\"first image : \",image,\", and there captions: \")\n",
        "    for c in captions:\n",
        "        print(c)\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aoanNqq8Ya_W"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "# Count all words in the dataset\n",
        "word_counter = Counter()\n",
        "\n",
        "for captions in image_captions.values():\n",
        "    for caption in captions:\n",
        "        words = caption.split()\n",
        "        word_counter.update(words)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZGkSfP5TYa_W"
      },
      "outputs": [],
      "source": [
        "print(len(word_counter))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "8orvboTXYa_X"
      },
      "outputs": [],
      "source": [
        "special_tokens = [\"<pad>\", \"<start>\", \"<end>\", \"<unk>\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "e3r-EA0jYa_X",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 220
        },
        "outputId": "06a23d39-ccbd-4d44-803e-7c531a7e8e70"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'word_counter' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3636879465.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Add remaining words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspecial_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mword_counter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mword2idx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mword2idx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'word_counter' is not defined"
          ]
        }
      ],
      "source": [
        "word2idx = {} # making a dictionary where word is assigned a unique number\n",
        "idx2word = {} # index to word\n",
        "\n",
        "# Add special tokens first\n",
        "for idx, token in enumerate(special_tokens):\n",
        "    word2idx[token] = idx\n",
        "    idx2word[idx] = token\n",
        "\n",
        "# Add remaining words\n",
        "idx = len(special_tokens)\n",
        "for word in word_counter:\n",
        "    if word not in word2idx:\n",
        "        word2idx[word] = idx\n",
        "        idx2word[idx] = word\n",
        "        idx += 1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u9PrgfSMYa_X"
      },
      "outputs": [],
      "source": [
        "vocab_size = len(word2idx)\n",
        "print(\"Vocabulary size:\", vocab_size)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jIBDy5vMYa_X"
      },
      "outputs": [],
      "source": [
        "print(idx2word[17]) # here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "shq9qD-xYa_X"
      },
      "outputs": [],
      "source": [
        "# Convert captions to sequences of numbers\n",
        "image_caption_sequences = {}\n",
        "\n",
        "for image, captions in image_captions.items():\n",
        "    seq_list = []\n",
        "    for caption in captions:\n",
        "        seq = []\n",
        "        for word in caption.split():\n",
        "            seq.append(word2idx.get(word, word2idx[\"<unk>\"]))\n",
        "        seq_list.append(seq)\n",
        "    image_caption_sequences[image] = seq_list\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0MrM7KDGYa_Y"
      },
      "outputs": [],
      "source": [
        "img = list(image_caption_sequences.keys())[0]\n",
        "print(\"Image:\", img)\n",
        "\n",
        "print(\"Text caption:\")\n",
        "print(image_captions[img][0])\n",
        "\n",
        "print(\"Sequence:\")\n",
        "print(image_caption_sequences[img][0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JSkIFe9BYa_Y"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "with open(\"flickr30k_features.pkl\", \"rb\") as f:\n",
        "    image_features = pickle.load(f)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"vocab.pkl\", \"wb\") as f:\n",
        "    pickle.dump({\n",
        "        \"word2idx\": word2idx,\n",
        "        \"idx2word\": idx2word\n",
        "    }, f)\n"
      ],
      "metadata": {
        "id": "XrH477FVM9pJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UXS4rCwsYa_Y"
      },
      "outputs": [],
      "source": [
        "print(type(image_features))\n",
        "print(len(image_features))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OZMO5gy1Ya_Y"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "all_images = list(image_caption_sequences.keys())\n",
        "\n",
        "# First split: Train + Temp (Val + Test)\n",
        "train_images, temp_images = train_test_split(\n",
        "    all_images,\n",
        "    test_size=0.3,      # 30% for val + test\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Second split: Validation + Test\n",
        "val_images, test_images = train_test_split(\n",
        "    temp_images,\n",
        "    test_size=2/3,      # 20% test, 10% val\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "print(len(train_images), len(val_images), len(test_images))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UIz4S8J1Ya_Y"
      },
      "outputs": [],
      "source": [
        "print(type(image_features))\n",
        "print(type(image_caption_sequences))\n",
        "print(len(train_images))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uyrT-mnzYa_Y"
      },
      "outputs": [],
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, image_features, caption_sequences, image_names):\n",
        "        self.image_features = image_features\n",
        "        self.caption_sequences = caption_sequences\n",
        "        self.image_names = image_names\n",
        "\n",
        "        # Build a flat list of (image, caption) pairs\n",
        "        self.data = []\n",
        "        for img in image_names:\n",
        "            for seq in caption_sequences[img]:\n",
        "                self.data.append((img, seq))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name, caption = self.data[idx]\n",
        "\n",
        "        img_feat = torch.tensor(self.image_features[img_name], dtype=torch.float32)\n",
        "\n",
        "        caption = torch.tensor(caption, dtype=torch.long)\n",
        "\n",
        "        # Input and target captions\n",
        "        input_caption = caption[:-1]\n",
        "        target_caption = caption[1:]\n",
        "\n",
        "        return img_feat, input_caption, target_caption\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w7d58pSRYa_Z"
      },
      "outputs": [],
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "PAD_IDX = word2idx[\"<pad>\"]\n",
        "\n",
        "def collate_fn(batch):\n",
        "    img_feats, inputs, targets = zip(*batch)\n",
        "\n",
        "    img_feats = torch.stack(img_feats)\n",
        "\n",
        "    inputs = pad_sequence(inputs, batch_first=True, padding_value=PAD_IDX)\n",
        "    targets = pad_sequence(targets, batch_first=True, padding_value=PAD_IDX)\n",
        "\n",
        "    return img_feats, inputs, targets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9HyfP4GTYa_Z"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Train dataset & loader\n",
        "train_dataset = CustomDataset(\n",
        "    image_features,\n",
        "    image_caption_sequences,\n",
        "    train_images\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=32,\n",
        "    shuffle=True,          # shuffle ONLY for training\n",
        "    collate_fn=collate_fn\n",
        ")\n",
        "\n",
        "# Validation dataset & loader\n",
        "val_dataset = CustomDataset(\n",
        "    image_features,\n",
        "    image_caption_sequences,\n",
        "    val_images\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=32,\n",
        "    shuffle=False,         # no shuffle for validation\n",
        "    collate_fn=collate_fn\n",
        ")\n",
        "\n",
        "# Test dataset & loader\n",
        "test_dataset = CustomDataset(\n",
        "    image_features,\n",
        "    image_caption_sequences,\n",
        "    test_images\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=32,\n",
        "    shuffle=False,         # no shuffle for test\n",
        "    collate_fn=collate_fn\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CRrohbICYa_Z"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim=2048, hidden_dim=512):\n",
        "        super().__init__()\n",
        "        self.fc = nn.Linear(input_dim, hidden_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, image_features):\n",
        "        # image_features: (batch_size, 2048)\n",
        "        x = self.fc(image_features)\n",
        "        x = self.relu(x)\n",
        "        return x  # (batch_size, hidden_dim)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XqHdMPRIYa_a"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim=256, hidden_dim=512,dropout=0.3):\n",
        "        super().__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        self.lstm = nn.LSTM(\n",
        "            embed_dim,\n",
        "            hidden_dim,\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "    def forward(self, captions, encoder_hidden):\n",
        "        \"\"\"\n",
        "        captions: (batch, seq_len)\n",
        "        encoder_hidden: (batch, hidden_dim)\n",
        "        \"\"\"\n",
        "\n",
        "        embeddings = self.embedding(captions)\n",
        "        embeddings = self.dropout(embeddings)\n",
        "\n",
        "        h0 = encoder_hidden.unsqueeze(0)   # (1, batch, hidden_dim)\n",
        "        c0 = torch.zeros_like(h0)\n",
        "\n",
        "        outputs, _ = self.lstm(embeddings, (h0, c0))\n",
        "        outputs = self.dropout(outputs)\n",
        "\n",
        "        outputs = self.fc(outputs)  # (batch, seq_len, vocab_size)\n",
        "        return outputs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L1gNalAzYa_a"
      },
      "outputs": [],
      "source": [
        "class ImageCaptioningModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim=256, hidden_dim=512):\n",
        "        super().__init__()\n",
        "\n",
        "        self.encoder = Encoder(2048, hidden_dim)\n",
        "        self.decoder = Decoder(\n",
        "            vocab_size=vocab_size,\n",
        "            embed_dim=embed_dim,\n",
        "            hidden_dim=hidden_dim\n",
        "        )\n",
        "\n",
        "    def forward(self, image_features, captions):\n",
        "        enc_out = self.encoder(image_features)\n",
        "        outputs = self.decoder(captions, enc_out)\n",
        "        return outputs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1fkloVCPYa_a"
      },
      "outputs": [],
      "source": [
        "def train_one_epoch(model, dataloader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for img_feats, captions_in, captions_out in dataloader:\n",
        "        img_feats = img_feats.to(device)\n",
        "        captions_in = captions_in.to(device)\n",
        "        captions_out = captions_out.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(img_feats, captions_in)\n",
        "        # outputs: (batch_size, seq_len, vocab_size)\n",
        "\n",
        "        loss = criterion(\n",
        "            outputs.reshape(-1, outputs.size(-1)),\n",
        "            captions_out.reshape(-1)\n",
        "        )\n",
        "\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5)\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(dataloader)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dr2ovUG9Ya_b"
      },
      "outputs": [],
      "source": [
        "def validate_one_epoch(model, dataloader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for img_feats, captions_in, captions_out in dataloader:\n",
        "            img_feats = img_feats.to(device)\n",
        "            captions_in = captions_in.to(device)\n",
        "            captions_out = captions_out.to(device)\n",
        "\n",
        "            outputs = model(img_feats, captions_in)\n",
        "\n",
        "            loss = criterion(\n",
        "                outputs.view(-1, outputs.size(-1)),\n",
        "                captions_out.view(-1)\n",
        "            )\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(dataloader)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ex8hoDIyYa_b"
      },
      "outputs": [],
      "source": [
        "# 1 Device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# 2 Vocabulary size\n",
        "vocab_size = len(word2idx)\n",
        "\n",
        "# 3  Model\n",
        "model = ImageCaptioningModel(\n",
        "    vocab_size=vocab_size,\n",
        "    embed_dim=256\n",
        "    ,\n",
        "    hidden_dim=512\n",
        ")\n",
        "model = model.to(device)\n",
        "\n",
        "# 4  Loss function\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=word2idx[\"<pad>\"])\n",
        "\n",
        "# 5 Optimizer\n",
        "optimizer = torch.optim.Adam(\n",
        "    model.parameters(),\n",
        "    lr=2e-4,\n",
        "    weight_decay=1e-5   # L2 regularization\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ew_pkN9oYa_c",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "### from tqdm import tqdm\n",
        "import copy\n",
        "\n",
        "num_epochs = 25\n",
        "patience = 3\n",
        "min_delta = 0.001\n",
        "\n",
        "best_val_loss = float(\"inf\")\n",
        "patience_counter = 0\n",
        "best_model_state = None\n",
        "\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "for epoch in tqdm(range(num_epochs), desc=\"Training Epochs\"):\n",
        "\n",
        "    train_loss = train_one_epoch(\n",
        "        model, train_loader, optimizer, criterion, device\n",
        "    )\n",
        "\n",
        "    val_loss = validate_one_epoch(\n",
        "        model, val_loader, criterion, device\n",
        "    )\n",
        "\n",
        "    train_losses.append(train_loss)\n",
        "    val_losses.append(val_loss)\n",
        "\n",
        "    tqdm.write(\n",
        "        f\"Epoch [{epoch+1}/{num_epochs}] \"\n",
        "        f\"Train Loss: {train_loss:.4f} | \"\n",
        "        f\"Val Loss: {val_loss:.4f}\"\n",
        "    )\n",
        "\n",
        "    # -------- Early Stopping Logic --------\n",
        "    if best_val_loss - val_loss > min_delta:\n",
        "        best_val_loss = val_loss\n",
        "        patience_counter = 0\n",
        "        best_model_state = copy.deepcopy(model.state_dict())\n",
        "        torch.save(best_model_state, \"best_model.pth\")\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "\n",
        "\n",
        "\n",
        "    if patience_counter >= patience:\n",
        "        tqdm.write(\"Early stopping triggered!\")\n",
        "        break\n",
        "\n",
        "# Load best model\n",
        "model.load_state_dict(best_model_state)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8NABVIX7Ya_c"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "epochs = range(1, len(train_losses) + 1)\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(epochs, train_losses, label=\"Training Loss\")\n",
        "plt.plot(epochs, val_losses, label=\"Validation Loss\")\n",
        "\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Training vs Validation Loss\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def greedy_caption(\n",
        "    model,\n",
        "    image_feature,\n",
        "    word2idx,\n",
        "    idx2word,\n",
        "    max_len=20,\n",
        "    device=\"cpu\"\n",
        "):\n",
        "    model.eval()\n",
        "\n",
        "    # Add batch dimension\n",
        "    image_feature = image_feature.unsqueeze(0).to(device)\n",
        "\n",
        "    # Encode image â†’ initial hidden state\n",
        "    with torch.no_grad():\n",
        "        encoder_hidden = model.encoder(image_feature)\n",
        "\n",
        "    # Start token\n",
        "    current_word = torch.tensor([[word2idx[\"<start>\"]]], device=device)\n",
        "\n",
        "    caption = []\n",
        "    hidden = None  # important for LSTM state passing\n",
        "\n",
        "    for _ in range(max_len):\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs, hidden = model.decoder(\n",
        "                current_word,\n",
        "                encoder_hidden,\n",
        "                hidden\n",
        "            )\n",
        "\n",
        "        # Last timestep logits\n",
        "        logits = outputs[:, -1, :]\n",
        "\n",
        "        # Pick highest probability word\n",
        "        predicted_idx = logits.argmax(dim=-1).item()\n",
        "        predicted_word = idx2word[predicted_idx]\n",
        "\n",
        "        # Stop if end token\n",
        "        if predicted_word == \"<end>\":\n",
        "            break\n",
        "\n",
        "        caption.append(predicted_word)\n",
        "\n",
        "        # Feed prediction back as next input\n",
        "        current_word = torch.tensor([[predicted_idx]], device=device)\n",
        "\n",
        "    return \" \".join(caption)\n"
      ],
      "metadata": {
        "id": "bgWnJGzRDSEz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jZjSHw4tYa_d"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def beam_search_caption(\n",
        "    model,\n",
        "    image_feature,\n",
        "    word2idx,\n",
        "    idx2word,\n",
        "    beam_width=3,\n",
        "    max_len=20,\n",
        "    device=\"cpu\"\n",
        "):\n",
        "    model.eval()\n",
        "\n",
        "    image_feature = image_feature.unsqueeze(0).to(device)\n",
        "\n",
        "    # Encode image once\n",
        "    with torch.no_grad():\n",
        "        encoder_hidden = model.encoder(image_feature)\n",
        "\n",
        "    # Each beam: (sequence, score)\n",
        "    sequences = [([word2idx[\"<start>\"]], 0.0)]\n",
        "\n",
        "    for _ in range(max_len):\n",
        "        all_candidates = []\n",
        "\n",
        "        for seq, score in sequences:\n",
        "\n",
        "            # Stop expanding if <end> already generated\n",
        "            if seq[-1] == word2idx[\"<end>\"]:\n",
        "                all_candidates.append((seq, score))\n",
        "                continue\n",
        "\n",
        "            caption_tensor = torch.tensor(\n",
        "                [seq], dtype=torch.long, device=device\n",
        "            )\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = model.decoder(caption_tensor, encoder_hidden)\n",
        "\n",
        "            logits = outputs[:, -1, :]\n",
        "            log_probs = F.log_softmax(logits, dim=-1)\n",
        "\n",
        "            topk_log_probs, topk_idxs = log_probs.topk(beam_width)\n",
        "\n",
        "            for i in range(beam_width):\n",
        "                next_word = topk_idxs[0][i].item()\n",
        "                next_score = score + topk_log_probs[0][i].item()\n",
        "\n",
        "                # repetition penalty\n",
        "                if next_word in seq:\n",
        "                    next_score -= 1.5\n",
        "\n",
        "                candidate = (seq + [next_word], next_score)\n",
        "                all_candidates.append(candidate)\n",
        "\n",
        "        sequences = sorted(\n",
        "            all_candidates, key=lambda x: x[1], reverse=True\n",
        "        )[:beam_width]\n",
        "\n",
        "    best_seq = sequences[0][0]\n",
        "\n",
        "    # Convert to words\n",
        "    caption = []\n",
        "    for idx in best_seq[1:]:\n",
        "        word = idx2word[idx]\n",
        "        if word == \"<end>\":\n",
        "            break\n",
        "        caption.append(word)\n",
        "\n",
        "    return \" \".join(caption)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pYJ5MdcxYa_d"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "# Your existing code\n",
        "img_name = test_images[2020]\n",
        "img_feat = torch.tensor(image_features[img_name], dtype=torch.float32)\n",
        "\n",
        "caption = beam_search_caption(\n",
        "    model,\n",
        "    img_feat,\n",
        "    word2idx,\n",
        "    idx2word,\n",
        "    beam_width=3,\n",
        "    device=device\n",
        ")\n",
        "\n",
        "print(\"Image:\", img_name)\n",
        "print(\"Beam Search Caption:\")\n",
        "print(caption)\n",
        "\n",
        "# -------- SHOW IMAGE --------\n",
        "img_path = os.path.join(IMAGE_DIR, img_name)\n",
        "image = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "plt.figure(figsize=(6, 6))\n",
        "plt.imshow(image)\n",
        "plt.axis(\"off\")\n",
        "plt.title(caption, fontsize=12)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ODSclvlwYa_n"
      },
      "outputs": [],
      "source": [
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "import torch\n",
        "\n",
        "def evaluate_bleu(\n",
        "    model,\n",
        "    image_features,\n",
        "    image_captions,\n",
        "    test_images,\n",
        "    word2idx,\n",
        "    idx2word,\n",
        "    device,\n",
        "    beam_width=5,\n",
        "    max_samples=1000\n",
        "):\n",
        "    references = []\n",
        "    hypotheses = []\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    # Use subset for speed\n",
        "    sample_images = random.sample(\n",
        "        test_images, min(max_samples, len(test_images))\n",
        "    )\n",
        "\n",
        "    for img_name in tqdm(sample_images, desc=\"Evaluating BLEU-4\"):\n",
        "        img_feat = torch.tensor(\n",
        "            image_features[img_name], dtype=torch.float32\n",
        "        ).to(device)\n",
        "\n",
        "        # Generate caption\n",
        "        pred_caption = beam_search_caption(\n",
        "            model,\n",
        "            img_feat,\n",
        "            word2idx,\n",
        "            idx2word,\n",
        "            beam_width=beam_width,\n",
        "            device=device\n",
        "        )\n",
        "\n",
        "        pred_tokens = pred_caption.split()\n",
        "        hypotheses.append(pred_tokens)\n",
        "\n",
        "        # Ground truth captions (multiple references)\n",
        "        gt_captions = image_captions[img_name]\n",
        "        gt_tokens = [\n",
        "            cap.replace(\"<start>\", \"\").replace(\"<end>\", \"\").split()\n",
        "            for cap in gt_captions\n",
        "        ]\n",
        "\n",
        "        references.append(gt_tokens)\n",
        "\n",
        "    bleu4 = corpus_bleu(\n",
        "        references,\n",
        "        hypotheses,\n",
        "        weights=(0.25, 0.25, 0.25, 0.25)\n",
        "    )\n",
        "\n",
        "    return bleu4\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rnocq-G7Ya_n"
      },
      "outputs": [],
      "source": [
        "bleu4 = evaluate_bleu(\n",
        "    model,\n",
        "    image_features,\n",
        "    image_captions,\n",
        "    test_images,\n",
        "    word2idx,\n",
        "    idx2word,\n",
        "    device,\n",
        "    beam_width=5\n",
        ")\n",
        "\n",
        "print(f\"BLEU-4 Score: {bleu4:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cnzJIb0nYa_o"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "def precision_recall_f1(pred_tokens, gt_tokens):\n",
        "    pred_counter = Counter(pred_tokens)\n",
        "    gt_counter = Counter(gt_tokens)\n",
        "\n",
        "    common = pred_counter & gt_counter\n",
        "    true_positive = sum(common.values())\n",
        "\n",
        "    precision = true_positive / max(len(pred_tokens), 1)\n",
        "    recall = true_positive / max(len(gt_tokens), 1)\n",
        "\n",
        "    if precision + recall == 0:\n",
        "        f1 = 0.0\n",
        "    else:\n",
        "        f1 = 2 * precision * recall / (precision + recall)\n",
        "\n",
        "    return precision, recall, f1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uNYcN-VUYa_o"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "import random\n",
        "import torch\n",
        "\n",
        "def evaluate_prf(\n",
        "    model,\n",
        "    image_features,\n",
        "    image_captions,\n",
        "    test_images,\n",
        "    word2idx,\n",
        "    idx2word,\n",
        "    device,\n",
        "    beam_width=5,\n",
        "    max_samples=500\n",
        "):\n",
        "    precisions, recalls, f1s = [], [], []\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    sample_images = random.sample(\n",
        "        test_images, min(max_samples, len(test_images))\n",
        "    )\n",
        "\n",
        "    for img_name in tqdm(sample_images, desc=\"Evaluating Precision/Recall/F1\"):\n",
        "        img_feat = torch.tensor(\n",
        "            image_features[img_name], dtype=torch.float32\n",
        "        ).to(device)\n",
        "\n",
        "        # Generate caption\n",
        "        pred_caption = beam_search_caption(\n",
        "            model,\n",
        "            img_feat,\n",
        "            word2idx,\n",
        "            idx2word,\n",
        "            beam_width=beam_width,\n",
        "            device=device\n",
        "        )\n",
        "\n",
        "        pred_tokens = pred_caption.split()\n",
        "\n",
        "        # Use first ground-truth caption\n",
        "        gt_caption = image_captions[img_name][0]\n",
        "        gt_tokens = gt_caption.replace(\"<start>\", \"\").replace(\"<end>\", \"\").split()\n",
        "\n",
        "        p, r, f = precision_recall_f1(pred_tokens, gt_tokens)\n",
        "\n",
        "        precisions.append(p)\n",
        "        recalls.append(r)\n",
        "        f1s.append(f)\n",
        "\n",
        "    return (\n",
        "        sum(precisions) / len(precisions),\n",
        "        sum(recalls) / len(recalls),\n",
        "        sum(f1s) / len(f1s),\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ue5torJWYa_o"
      },
      "outputs": [],
      "source": [
        "precision, recall, f1 = evaluate_prf(\n",
        "    model,\n",
        "    image_features,\n",
        "    image_captions,\n",
        "    test_images,\n",
        "    word2idx,\n",
        "    idx2word,\n",
        "    device\n",
        ")\n",
        "\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall:    {recall:.4f}\")\n",
        "print(f\"F1-score:  {f1:.4f}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kaggle": {
      "accelerator": "tpuV5e8",
      "dataSources": [
        {
          "datasetId": 623329,
          "sourceId": 1111749,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 31260,
      "isGpuEnabled": false,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}